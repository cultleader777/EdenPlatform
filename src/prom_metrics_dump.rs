use std::{error::Error, collections::BTreeMap};

use futures::future::join_all;
use serde::{Deserialize, Serialize};

#[cfg(test)]
use serde_json::json;

use crate::codegen::MetricScrapePlan;

struct PrometheusCluster {
    cluster_name: String,
    cluster_url: String,
}

pub fn refresh_prometheus_metrics(prometheus_urls: &[String]) {
    let clusters = prometheus_urls.iter().map(|prom_url| {
        let spl = prom_url.split(",").collect::<Vec<_>>();
        if spl.len() != 2 {
            panic!("Expected prometheus cluster name and url separated by comma, got: [{prom_url}]");
        }
        PrometheusCluster {
            cluster_name: spl[0].to_string(),
            cluster_url: spl[1].to_string(),
        }
    }).collect::<Vec<_>>();

    let rt =
        tokio::runtime::Builder::new_current_thread()
            .enable_io()
            .build()
            .expect("Can't create tokio runtime");

    match rt.block_on(refresh(&clusters)) {
        Ok(res) => {
            println!("# DO NOT EDIT THIS FILE BY HAND
# this is refreshed by 'make refresh-metrics-db'
{res}")
        }
        Err(e) => {
            panic!("Can't dump prometheus metrics: {:?}", e)
        }
    }
}

#[derive(Debug, Clone)]
struct Non200PrometheusResponseCodeError(u16);

impl std::fmt::Display for Non200PrometheusResponseCodeError {
    fn fmt(&self, f: &mut std::fmt::Formatter) -> std::fmt::Result {
        write!(f, "non 200 response code returned from prometheus: {}", self.0)
    }
}

impl Error for Non200PrometheusResponseCodeError {}

pub fn metrics_db_schema() -> &'static str {
    "
CREATE TABLE IF NOT EXISTS metrics (
  timestamp TEXT DEFAULT CURRENT_TIMESTAMP,
  cluster TEXT,
  metric_name TEXT,
  metric_expression TEXT,
  mirror TEXT,
  value TEXT,
  UNIQUE(cluster, metric_name)
);

CREATE TABLE IF NOT EXISTS metrics_log (
  timestamp TEXT DEFAULT CURRENT_TIMESTAMP,
  cluster TEXT,
  metric_name TEXT,
  metric_expression TEXT,
  mirror TEXT,
  value TEXT
);
"
}

async fn fetch_prom_metrics_and_dump(metrics_plan: &[MetricScrapePlan], sqlite_db: &str) {

    let mut futs = Vec::with_capacity(16);
    let client = reqwest::Client::builder()
        .timeout(std::time::Duration::from_millis(5000))
        .build()
        .expect("Can't build");

    #[derive(Clone)]
    pub struct ScrapeResult {
        pub cluster_name: String,
        pub metric_name: String,
        pub expression: String,
        pub mirror: String,
        pub body: String,
    }

    for mp in metrics_plan {
        for scrape in &mp.scrapes {
            let client = client.clone();
            let mirrors = mp.mirrors.clone();
            let scrape = scrape.clone();
            let cluster_name = mp.cluster_name.clone();
            futs.push(tokio::spawn(async move {
                let mut last_err: Option<Box<dyn Error + Send>> = None;
                for mirror in mirrors {
                    let url = format!("{}/api/v1/query", mirror);
                    let res = client.get(url)
                        .query(&[("query", &scrape.expression)])
                        .send()
                        .await;


                    match res {
                        Ok(res) => {
                            if !res.status().is_success() {
                                last_err = Some(Box::new(Non200PrometheusResponseCodeError(res.status().as_u16())));
                                continue;
                            }
                            let body = res.text().await;
                            match body {
                                Ok(body) => {
                                    return Ok(ScrapeResult {
                                        cluster_name,
                                        body,
                                        mirror,
                                        metric_name: scrape.name,
                                        expression: scrape.expression,
                                    });
                                }
                                Err(err) => {
                                    last_err = Some(Box::new(err));
                                }
                            }
                        }
                        Err(err) => {
                            last_err = Some(Box::new(err))
                        }
                    }
                }

                return Err(last_err.unwrap());
            }));
        }
    }

    let joined = futures::future::join_all(futs).await;

    let sqlite_db = ::rusqlite::Connection::open(sqlite_db).expect("Can't open sqlite database");
    sqlite_db.execute_batch(metrics_db_schema()).expect("Can't perform schema migration");
    sqlite_db.execute("BEGIN;", []).expect("failed to begin");
    for j in joined.iter() {
        match j {
            Ok(ok) => {
                match ok {
                    Ok(ok) => {
                        // save previous metric if it exists
                        sqlite_db.execute("
                            INSERT INTO metrics_log(cluster, metric_name, metric_expression, mirror, value, timestamp)
                            SELECT cluster, metric_name, metric_expression, mirror, value, timestamp
                            FROM metrics
                            WHERE cluster=? AND metric_name=?
                        ", [
                            &ok.cluster_name,
                            &ok.metric_name,
                        ]).expect("failed storing to log");

                        sqlite_db.execute("
                            INSERT INTO metrics(cluster, metric_name, metric_expression, mirror, value)
                            VALUES(?, ?, ?, ?, ?)
                            ON CONFLICT(cluster, metric_name)
                            DO UPDATE
                            SET
                              metric_expression=excluded.metric_expression,
                              mirror=excluded.mirror,
                              value=excluded.value,
                              timestamp=CURRENT_TIMESTAMP
                        ", [
                            &ok.cluster_name,
                            &ok.metric_name,
                            &ok.expression,
                            &ok.mirror,
                            &ok.body,
                        ]).expect("failed insertion");
                    }
                    Err(err) => {
                        eprintln!("Failed to query scrape metric: {err}")
                    }
                }
            }
            Err(err) => {
                eprintln!("Failed to query metric: {err}")
            }
        }
    }
    sqlite_db.execute("COMMIT;", []).expect("failed to commit");
}

pub fn dump_prometheus_metrics(metrics_plan: &[MetricScrapePlan], sqlite_db: &str) {
    let rt =
        tokio::runtime::Builder::new_current_thread()
            .enable_io()
            .enable_time()
            .build()
            .expect("Can't create tokio runtime");

    rt.block_on(fetch_prom_metrics_and_dump(metrics_plan, sqlite_db))
}

#[derive(Deserialize)]
struct PromResultNode {
    metric: BTreeMap<String, String>,
    value: [serde_json::Value; 2],
}

#[derive(Deserialize)]
struct PromData {
    #[serde(rename(deserialize = "resultType"))]
    result_type: String,

    result: Vec<PromResultNode>,
}

#[derive(Deserialize)]
struct PromResponse {
    status: String,
    data: PromData,
}

#[derive(Deserialize, Serialize, Debug)]
pub struct OutputPromMetric {
    // min/max sample of metric
    pub samples: Vec<String>,
    // min/max sample of labels
    pub labels: BTreeMap<String, Vec<String>>,
}

pub type PromSeriesDatabase = BTreeMap<String, OutputPromMetric>;
pub type AllClusterSeriesDatabase = BTreeMap<String, PromSeriesDatabase>;

impl OutputPromMetric {
    pub fn merge(&mut self, other: &OutputPromMetric) {
        for s in &other.samples {
            label_vec_min_max_assign(&mut self.samples, s.as_str());
        }
        for (label_key, label_values) in &other.labels {
            let labels = self.labels.entry(label_key.clone()).or_insert_with(|| { Vec::new() });
            for label_value in label_values {
                label_vec_min_max_assign(labels, label_value.as_str());
            }
        }
    }
}

async fn refresh(clusters: &[PrometheusCluster]) -> Result<String, Box<dyn Error>> {
    let mut futures: Vec<_> = Vec::with_capacity(clusters.len());
    let mut res = BTreeMap::new();

    for cluster in clusters {
        futures.push(refresh_single_cluster(cluster));
    }

    let joined = join_all(futures).await;

    for (idx, i) in joined.iter().enumerate() {
        match i {
            Ok(ok) => {
                assert!(res.insert(clusters[idx].cluster_name.clone(), ok).is_none());
            }
            Err(err) => {
                panic!("Failed to query cluster {}: {}", clusters[idx].cluster_name, err);
            }
        }
    }

    let serialized = serde_yaml::to_string(&res).expect("Can't serialize yaml");
    Ok(serialized)
}

async fn refresh_single_cluster(cluster: &PrometheusCluster) -> Result<PromSeriesDatabase, Box<dyn Error>> {
    let prometheus_url = &cluster.cluster_url;
    let enc_q = urlencoding::encode("{__name__!=\"\"}");
    let res =
        reqwest::get(format!("{prometheus_url}/api/v1/query?query={enc_q}"))
            .await?;

    if !res.status().is_success() {
        panic!("Bad response code from prometheus server: {}", res.status().as_str());
    }

    let body = res.bytes().await?.to_vec();

    let deser: PromResponse = serde_json::from_slice(&body)?;

    assert_eq!(deser.status, "success");
    assert_eq!(deser.data.result_type, "vector");

    Ok(prom_input_to_label_db(&deser.data.result))
}

fn prom_input_to_label_db(rn: &[PromResultNode]) -> BTreeMap<String, OutputPromMetric> {
    let mut label_store: BTreeMap<String, OutputPromMetric> = BTreeMap::new();

    for dd in rn {
        let _time = &dd.value[0];
        let value = &dd.value[1];
        match value {
            serde_json::Value::String(value) => {
                let name = dd.metric.get("__name__").expect("Can't find __name__ label");
                for (label_key, label_value) in &dd.metric {
                    if label_key != "__name__" {
                        let e = label_store.entry(name.clone()).or_insert_with(|| {
                            OutputPromMetric {
                                labels: BTreeMap::new(),
                                samples: Vec::new(),
                            }
                        });
                        // Keep minimum and maximum label for reference
                        let labels = e.labels.entry(label_key.clone()).or_insert_with(|| { Vec::new() });
                        label_vec_min_max_assign(labels, label_value.as_str());
                        label_vec_min_max_assign(&mut e.samples, value);
                    }
                }
            }
            v => {
                panic!("Unexpected prometheus metric value type: {:?}", v)
            }
        }
    }

    label_store
}

pub fn merge_databases(res: &mut BTreeMap<String, OutputPromMetric>, input: &BTreeMap<String, OutputPromMetric>) {
    for (name, v) in input {
        let e = res.entry(name.clone()).or_insert_with(|| {
            OutputPromMetric {
                labels: BTreeMap::new(),
                samples: Vec::new(),
            }
        });
        e.merge(v);
    }
}

fn label_vec_min_max_assign(inp: &mut Vec<String>, v: &str) {
    // this branch occours most often that's why it's first
    if inp.len() == 2 {
        if v >= inp[0].as_str() && v <= inp[1].as_str() {
            // do nothing, label falls in range
        } else if v > inp[1].as_str() {
            // maximum label higher, assign
            inp[1] = v.to_string();
        } else if v < inp[0].as_str() {
            // minimum label lower, assign
            inp[0] = v.to_string();
        } else {
            panic!("This branch should never be reached.");
        }
    } else if inp.is_empty() {
        inp.push(v.to_string());
    } else if inp.len() == 1 {
        if inp[0] != v {
            if v > inp[0].as_str() {
                inp.push(v.to_string());
            } else {
                inp.insert(0, v.to_string());
            }
        }
    } else {
        panic!("This branch should never be reached");
    }
}

#[test]
fn test_min_max_assign() {
    let mut v = Vec::new();
    label_vec_min_max_assign(&mut v, "e");
    assert_eq!(v, vec!["e".to_string()]);
    label_vec_min_max_assign(&mut v, "e");
    assert_eq!(v, vec!["e".to_string()]);
    label_vec_min_max_assign(&mut v, "f");
    assert_eq!(v, vec!["e".to_string(), "f".to_string()]);
    v.pop().unwrap();
    label_vec_min_max_assign(&mut v, "d");
    assert_eq!(v, vec!["d".to_string(), "e".to_string()]);
    label_vec_min_max_assign(&mut v, "d");
    assert_eq!(v, vec!["d".to_string(), "e".to_string()]);
    label_vec_min_max_assign(&mut v, "e");
    assert_eq!(v, vec!["d".to_string(), "e".to_string()]);
    label_vec_min_max_assign(&mut v, "f");
    assert_eq!(v, vec!["d".to_string(), "f".to_string()]);
    label_vec_min_max_assign(&mut v, "c");
    assert_eq!(v, vec!["c".to_string(), "f".to_string()]);
}

#[test]
fn test_prom_to_db_conversion() {
    let input = r#"
{
    "status": "success",
    "data": {
        "resultType": "vector",
        "result": [
            {
                "metric": {
                    "__name__": "access_evaluation_duration_bucket",
                    "instance": "server-a:3000",
                    "job": "epl-grafana-main",
                    "le": "+Inf"
                },
                "value": [
                    1685884643.602,
                    "45"
                ]
            },
            {
                "metric": {
                    "__name__": "access_evaluation_duration_bucket",
                    "instance": "server-a:3000",
                    "job": "epl-grafana-main",
                    "le": "0.00016"
                },
                "value": [
                    1685884643.602,
                    "44"
                ]
            },
            {
                "metric": {
                    "__name__": "access_evaluation_duration_bucket",
                    "instance": "server-a:3000",
                    "job": "epl-grafana-main",
                    "le": "0.00064"
                },
                "value": [
                    1685884643.602,
                    "44"
                ]
            },
            {
                "metric": {
                    "__name__": "cadvisor_version_info",
                    "cadvisorVersion": "0.46.0",
                    "instance": "server-c:9280",
                    "job": "cadvisor",
                    "kernelVersion": "5.15.85",
                    "osVersion": "NixOS 22.11 (Raccoon)"
                },
                "value": [
                    1685884643.602,
                    "1"
                ]
            },
            {
                "metric": {
                    "__name__": "cadvisor_version_info",
                    "cadvisorVersion": "0.46.0",
                    "instance": "server-d:9280",
                    "job": "cadvisor",
                    "kernelVersion": "5.15.85",
                    "osVersion": "NixOS 22.11 (Raccoon)"
                },
                "value": [
                    1685884643.602,
                    "1"
                ]
            },
            {
                "metric": {
                    "__name__": "vector_utilization",
                    "component_id": "prometheus_exporter_sink",
                    "component_kind": "sink",
                    "component_name": "prometheus_exporter_sink",
                    "component_type": "prometheus_exporter",
                    "host": "nixos",
                    "instance": "server-d:9281",
                    "job": "vector"
                },
                "value": [
                    1685884643.602,
                    "0.00021289875738623844"
                ]
            },
            {
                "metric": {
                    "__name__": "vector_utilization",
                    "component_id": "provisioning_logs_extra",
                    "component_kind": "transform",
                    "component_name": "provisioning_logs_extra",
                    "component_type": "remap",
                    "host": "nixos",
                    "instance": "server-a:9281",
                    "job": "vector"
                },
                "value": [
                    1685884643.602,
                    "0.00001943725596393047"
                ]
            }
        ]
    }
}
    "#;
    let parsed: PromResponse = serde_json::from_str(input).unwrap();
    let db = prom_input_to_label_db(&parsed.data.result);
    let ser = serde_json::to_value(db).unwrap();
    pretty_assertions::assert_eq!(
        json!(
            {
                "access_evaluation_duration_bucket": {
                    "labels": {
                        "instance": [
                            "server-a:3000"
                        ],
                        "job": [
                            "epl-grafana-main"
                        ],
                        "le": [
                            "+Inf",
                            "0.00064",
                        ]
                    },
                    "samples": [
                        "44",
                        "45"
                    ]
                },
                "cadvisor_version_info": {
                    "labels": {
                        "cadvisorVersion": ["0.46.0"],
                        "instance": ["server-c:9280", "server-d:9280"],
                        "job": ["cadvisor"],
                        "kernelVersion": ["5.15.85"],
                        "osVersion": ["NixOS 22.11 (Raccoon)"],
                    },
                    "samples": [
                        "1",
                    ]
                },
                "vector_utilization": {
                    "labels": {
                        "component_id": ["prometheus_exporter_sink", "provisioning_logs_extra"],
                        "component_kind": ["sink", "transform"],
                        "component_name": ["prometheus_exporter_sink", "provisioning_logs_extra"],
                        "component_type": ["prometheus_exporter", "remap"],
                        "host": ["nixos"],
                        "instance": ["server-a:9281", "server-d:9281"],
                        "job": ["vector"],
                    },
                    "samples": [
                        "0.00001943725596393047",
                        "0.00021289875738623844",
                    ]
                }
            }
        ),
        ser
    );
}

#[test]
fn test_metrics_merge() {
    let mut a: BTreeMap<String, OutputPromMetric> = serde_json::from_value(json!({
        "metric_a": {
            "labels": {
                "label_a": ["a"],
                "label_b": ["a", "d"],
            },
            "samples": [
                "1",
                "2",
            ]
        }
    })).unwrap();
    let b: BTreeMap<String, OutputPromMetric> = serde_json::from_value(json!({
        "metric_a": {
            "labels": {
                "label_a": ["f"],
                "label_b": ["b", "z"],
            },
            "samples": [
                "8",
                "9",
            ]
        },
        "metric_b": {
            "labels": {
                "label_c": ["f"],
            },
            "samples": [
                "100",
                "200",
            ]
        }
    })).unwrap();

    merge_databases(&mut a, &b);
    let ser = serde_json::to_value(a).unwrap();
    pretty_assertions::assert_eq!(
        ser,
        json!({
            "metric_a": {
                "labels": {
                    "label_a": ["a", "f"],
                    "label_b": ["a", "z"],
                },
                "samples": [
                    "1",
                    "9",
                ]
            },
            "metric_b": {
                "labels": {
                    "label_c": ["f"],
                },
                "samples": [
                    "100",
                    "200",
                ]
            }
        })
    )
}
